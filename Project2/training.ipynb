{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy.stats\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import random as rand\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Our network\n",
    "class Policy_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy_Net,self).__init__()\n",
    "        self.input_layer = nn.Linear(env.observation_space.shape[0],28)\n",
    "        \n",
    "        self.action_layer = nn.Linear(28,env.action_space.n)\n",
    "        self.Value_layer = nn.Linear(28,1)\n",
    "        \n",
    "        self.ln_proba = []\n",
    "        self.state_val = []\n",
    "        self.reward_vec = []\n",
    "        \n",
    "    def forward(self,state):\n",
    "        state = self.input_layer(state)\n",
    "        state = F.dropout(state, p =.3)\n",
    "        state = F.relu(state)\n",
    "        \n",
    "        state_value = self.Value_layer(state)\n",
    "\n",
    "     \n",
    "        self.prob_d = F.softmax(self.action_layer(state), dim =1)\n",
    "        self.m = Categorical(self.prob_d)\n",
    "        self.action = self.m.sample()\n",
    "        \n",
    "        ## m.log_prob takes the natural log of the probability associated with action\n",
    "        self.ln_proba.append(self.m.log_prob(self.action))\n",
    "        ## approximate the Value of a given state\n",
    "        self.state_val.append(state_value)\n",
    "\n",
    "        \n",
    "\n",
    "        return self.action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "Gamma = 0.99\n",
    "## to reproduce graphs, leave the seed_value \n",
    "seed_value = 57899\n",
    " \n",
    "#seed_value = rand.randint(1,100000)\n",
    "\n",
    "learning_rate=.003\n",
    "torch.manual_seed(seed_value)\n",
    "plot_vec = []\n",
    "plot_vec2 = []\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "#env = gym.make(\"CartPole-v1\")\n",
    "env.seed(seed_value)\n",
    "policy = Policy_Net()\n",
    "policy.to(device)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "max_episodes = sys.maxsize\n",
    "See_results_after_traning = True\n",
    "plotting_average_after_x_datapoints = 100\n",
    "plot_vec = []\n",
    "entropy_vec = []\n",
    "entropy_vec2 = []\n",
    "entropy_plot = []\n",
    "\n",
    "\n",
    "\n",
    "## starts from 1 to infinity\n",
    "for episodes in range(max_episodes):\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "    ## start an episode\n",
    "    for t in range(1, 10000):\n",
    "        \n",
    "        \n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state = state.to(device)\n",
    "        ## feed a state to the neural netowkr\n",
    "        action = policy(state)\n",
    "        ## get entropy for a a state's probability dist.\n",
    "        entropy_vec.append(scipy.stats.entropy(policy.prob_d.detach().numpy()[0]))\n",
    "        ## take an action\n",
    "        action = action.item()\n",
    "        state, reward, done, _ = env.step(action)       \n",
    "        policy.reward_vec.append(reward)\n",
    "        ## accumulate reward  for graphs later\n",
    "        ep_reward += reward\n",
    "        if done:                    \n",
    "            plot_vec.append(ep_reward)\n",
    "            break\n",
    "            \n",
    "    # take the average entropy of the last game    \n",
    "    entropy_vec2= sum(entropy_vec)/len(entropy_vec)\n",
    "    \n",
    "    ## after every 100 episodes, give statistics\n",
    "    if len(plot_vec) == plotting_average_after_x_datapoints:\n",
    "        plot_vec2.append(np.average(plot_vec))\n",
    "\n",
    "        print(\"Learning rate =\",learning_rate,\"Seed Value=\", seed_value,\"Game #\",episodes+1,\"Entropy =\", np.round(entropy_vec2,3),\"Length of game=\",\n",
    "          len(policy.reward_vec),\"Total Reward =\", np.round(plot_vec2[-1],3))\n",
    "\n",
    "        entropy_plot.append(entropy_vec2)\n",
    "        \n",
    "        ## every 500 games, plot a graph of ep_reward and entropy\n",
    "        if len(entropy_plot)%5 ==0:\n",
    "            \n",
    "        \n",
    "            fig = plt.figure()\n",
    "            fig.suptitle('Reward')\n",
    "            plt.plot(plot_vec2)\n",
    "            plt.show()\n",
    "            print(\"Learning rate =\",learning_rate,\"Seed Value=\", seed_value,\"Game #\",episodes+1,\"Entropy =\", np.round(entropy_vec2,3),\"Length of game=\",\n",
    "          len(policy.reward_vec),\"Total Reward =\", np.round(plot_vec2[-1],3))\n",
    "\n",
    "            fig = plt.figure()\n",
    "            plt.plot(entropy_plot)\n",
    "            \n",
    "            fig.suptitle('Entropy')\n",
    "            plt.show()\n",
    "\n",
    "            \n",
    "            plt.close('all')\n",
    "            \n",
    "        \n",
    "\n",
    "        plot_vec=[]\n",
    "  \n",
    "        \n",
    "    ## finish episode\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    reward = []\n",
    "    value_losses = []\n",
    "    for r , value in zip(reversed(policy.reward_vec), reversed(policy.state_val)):\n",
    "        ## G_t  - Q(s,a)\n",
    "        R = r+(Gamma) *R - value \n",
    "        reward.insert(0,R)\n",
    "   \n",
    "        \n",
    "    reward = torch.tensor(reward)\n",
    "    ## standarize the rewards\n",
    "    reward = (reward - reward.mean()) / (reward.std())\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for ln_prob, reward, value in zip(policy.ln_proba,reward, policy.state_val):    \n",
    "        ## compute the loss function, J(theta)\n",
    "        action_loss = -ln_prob*reward       \n",
    "        loss+=action_loss \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    ## back propgate\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "    policy.reward_vec = []\n",
    "    policy.state_val = []\n",
    "    policy.ln_proba = []\n",
    "    entropy_vec = []\n",
    "    \n",
    "\n",
    "## we are done , let's play\n",
    "    \n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "\n",
    "#torch.save(policy.state_dict(), 'model_8')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##At anytime, you could stop training and run this cell to see current result\n",
    "while True:\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        state.to(device)\n",
    "        probs = policy(state)\n",
    "        \n",
    "        action = probs.item()\n",
    "\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        ep_reward+=reward\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            print(ep_reward)\n",
    "            ep_reward=0\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
